# -*- coding: utf-8 -*-
"""Bank_churn_prediction_Tensorflow

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/bank-churn-prediction-tensorflow-46bb7578-abbc-490d-bb68-d65da7b43c3d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240801/auto/storage/goog4_request%26X-Goog-Date%3D20240801T121255Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3199610447ec8e6f6a03be2d7d04d65efeb58a4a36323b517b3ebcaaef1910346e4cbdcdfa9c16415109e3b60342380be2f133834b4b249b9910f3bc51408504e3e32af71c81e438edcc6f23d52a6b97578473960a358a486d67b0c353f132833cd44fcc08150a248acc87e51d36bd0b02c18063216fcfc1ec6e54f4b50c4cd9ce2d2491766164d5cc2293df34a7d04b8afb8a477eadc47c5aa49efcfcfde5bda4790913a164f8b051bc33b9e53d8e496f34c5fdae462960893f3adffa4316d9aa9e0b725e8af4a251aeba013667d0c5b1e0608e3b5fcd329b056b293ba7906f51ad7fdb3c6ce7d07d3cc6246a6c6c863277b5a15fc73b2b3fd77b46b3ab7953
"""

# Commented out IPython magic to ensure Python compatibility.
# data manipulation
import pandas as pd
import numpy as np

# visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_theme(style='whitegrid', palette='viridis')

# preprocessing
from sklearn.preprocessing import StandardScaler, LabelEncoder

# deep learning
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

# path
import time
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/Churn_Modelling.csv")

df.head()

df.drop(columns = ['RowNumber', "CustomerId","Surname"], inplace=True, axis=1)

df.shape

from skimpy import skim
skim(df)

def summary(df):
    summary_df = pd.DataFrame(df.dtypes, columns=['dtypes'])
    summary_df['count'] = df.count().values
    summary_df['unique'] = df.nunique().values
    return summary_df

summary(df).style.background_gradient(cmap='Purples')

df.duplicated().sum()

def cleaned(df):
    # Encoding Categorical Features
    le = LabelEncoder()
    df['Geography'] = le.fit_transform(df['Geography'])
    df['Gender'] = le.fit_transform(df['Gender'])

    # Normalize Numerical Features
    num_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']
    sc = StandardScaler()
    df[num_features] = sc.fit_transform(df[num_features])

    return df

df0 = df.copy()

df_cleaned = cleaned(df)
df_cleaned.head()

X = df_cleaned.drop('Exited', axis=1)
y = df_cleaned['Exited']

X.head()

y.head()

plt.figure(figsize = [15,8])
fig = sns.heatmap(df_cleaned.corr(), cmap='hot_r',
                 annot=True, linecolor='black',linewidths=0.01, annot_kws={"fontsize":12}, fmt="0.2f")

top, bottom = fig.get_ylim()
fig.set_ylim(top+0.1,bottom-0.1)

left, right = fig.get_xlim()
fig.set_xlim(left-0.1,right+0.1)

plt.yticks(fontsize=13,rotation=0)
plt.xticks(fontsize=13,rotation=90)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state =42)

tf.random.set_seed(42)
epoch = 130

# Early stopping callback configuration
early_stopping = EarlyStopping(
    monitor='val_auc_metric',   # Metric to monitor for improvement
    min_delta=0.0001,           # Minimum change to qualify as an improvement
    patience=5,                 # Number of epochs to wait for improvement before stopping
    start_from_epoch=20,        # Start monitoring after this many epochs
    restore_best_weights=True,   # Restore model weights from the epoch with the best value of the monitored metric
    mode = 'max'
)

model = models.Sequential([
    layers.BatchNormalization(input_shape=[X.shape[1]]),
    layers.Dense(128, activation='leaky_relu', use_bias=True),
    layers.Dense(452, activation='relu', use_bias=True),
    layers.Dense(362, activation='relu', use_bias=True),
    layers.Dropout(0.4),
    layers.Dense(256, activation='relu', use_bias=True),
    layers.Dense(387, activation='leaky_relu', use_bias=True),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu', use_bias=True),
    layers.Dense(452, activation='relu', use_bias=True),
    layers.Dense(362, activation='relu', use_bias=True),
    layers.Dropout(0.4),
    layers.Dense(256, activation='leaky_relu', use_bias=True),
    layers.Dense(128, activation='relu', use_bias=True),
    layers.Dense(64, activation='leaky_relu', use_bias=True),
    layers.Dense(32, activation='relu', use_bias=True),
    layers.Dense(1, activation='sigmoid', use_bias=True)
])

model.compile(
    optimizer = 'adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy']
)

model.summary()

def model_evaluation_plot(history):
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    plt.plot(history.history['binary_accuracy'], label='Training Accuracy', color='orange')

    plt.title('Training Loss and Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Metrics')
    plt.legend()
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# history = model.fit(
#     X_train, y_train,
#     epochs=epoch,
#     validation_data=(X_test, y_test),
#     callbacks=[early_stopping]
# )

model_evaluation_plot(history)

loss, accuracy=model.evaluate(X_test, y_test)
print(accuracy, loss)

loss, accuracy=model.evaluate(X_train, y_train)
print(accuracy, loss)

